# -*- coding: utf-8 -*-
"""Assign5_Q1(50_Startups).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FdxYBO_yXLWuU-q6MdH7H2wqfCbCk9UP
"""

import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stat
import statsmodels.api as smf
import statsmodels.formula.api as sm

"""### **IMPORTING DATA**"""

from google.colab import files
df = files.upload()

df = pd.read_csv("50_Startups.csv")
df.head()

"""### **DESCRIPTIVE ANALYSIS**"""

df.describe()   #There are no null values

df.info() 
df.dtypes    #checking Datatypes

"""*Since state is a Categorical feature we need to do some encoding or convert it into numerical values.*"""

df1=df.rename({"R&D Spend":'RnD',"Administration":'admin',"Marketing Spend":'marketing',"State":'state',"Profit":'profit'},axis=1)       #Renaming columns
df1.head()

df1.isnull().sum()       #checking for null values

"""### *Note : There are no null values in our dataset*"""

df.duplicated()        #checking for duplicated values

"""### *Note : There are no duplicated values in our dataset.*

### **DETECTING OUTLIERS**
"""

continuous_variable=[feature for feature in df1.columns if df1[feature].dtype!='O']

for feature in continuous_variable:
    df1.boxplot(column=feature)
    plt.title(feature)
    plt.show()

"""***Note : As we can see, there is an outlier in our dependent variable, Profit. In bigger datasets, outliers should be handled because the overall mean or standard deviation gets affected if the outliers are not removed which leads to redundancy in output of the model. However, outliers can be kept in smaller datasets to prevent data loss.***

OUTLIERS CAN BE REMOVED USING:

1.Median Imputation

2.Mean Imputation

3.Using IQR

Transforming variables using sqrt,cuberoot or log transformations can also eliminate outliers.

### **Building a model without removing outliers**
"""

model1 = sm.ols("profit~RnD+admin+marketing", data = df1).fit()
# Finding rsquared values
model1.rsquared , model1.rsquared_adj

"""### **REMOVING OUTLIERS**

**By IQR**
"""

Q1 = np.quantile(df1.profit,0.25)
Q3 = np.quantile(df1.profit,0.75)
med = np.median(df1.profit)
IQR = Q3 - Q1

upper_bound = Q3+(1.5*IQR)
lower_bound = Q1-(1.5*IQR)
print('First Quantile=', Q1,'\n' 'Second Quantile=', med,'\n' 'Third Quantile=', Q3,'\n'
      'Inter-Quartile Range=', IQR,'\n' 'Upper Whisker=', upper_bound,'\n' 'Lower Whisker=', lower_bound)

# identify outliers
outliers = [x for x in df1.profit if x < lower_bound or x > upper_bound]
print("Outliers:",outliers)

display(df1[df1.index.isin([49])])

df2=df1.drop(df1.index[[49]],axis=0).reset_index(drop=True)
df2

"""### **Let's build one more model now again after removing the outliers.**"""

outlierless_model = sm.ols("profit~RnD+admin+marketing", data = df2).fit()
# Finding rsquared values
outlierless_model.rsquared , outlierless_model.rsquared_adj

"""### **As we can see, the accuracy of the outlier less model improved as compared to the previous model. So, let us continue with this model.**"""

sns.boxplot(df2.profit)
plt.title('Profit after removing outliers')

"""### **EXPLORATORY DATA ANALYSIS**

*Visualizing the Relation between each independent Feature with respect to the Dependent Feature*
"""

for feature in continuous_variable:
    if feature!="profit":        
        plt.scatter(df2[feature],df2['profit'])
        plt.xlabel(feature)
        plt.ylabel('profit')
        plt.title(feature)
        plt.show()

"""**Note: R&D feature has a good linear relation with Profit as compared to other features**

### **Correlation Analysis:**
"""

df1.corr()

"""### **Notes:**

**1.As we can see, there is high collinearity between Profit and RnD. So, that variable will be considered first in model building which will be followed by Marketing and then admin. (Highest collinearity comes first)**

**2.Also, there is high collinearity existing between two independent variables RnD and Marketing. So, we shouldn't build a model where two of those variables exist.**

### **VISUALISING CORRELATION**
"""

sns.set_style(style='darkgrid')
sns.pairplot(df1)

"""**1. As we can see in the pair plot, Research and development are directly proportional to the investment that we can do.**

**2. The marketing spend seems to be directly proportional (though a little bit outliers are there) with the profit.**

**3. There is no relationship between the second column and profit i.e. our target column.**

### **Preparing a Model**
"""

import statsmodels.formula.api as sm
model = sm.ols("profit~RnD+admin+marketing", data = df1).fit()
model.summary()

"""### **Model Testing**

**Finding Coefficient Parameters (Beta0 and Beta1's values)**
"""

# Finding Coefficient parameters
model.params

"""Assupmtion for multi linear Regression fails.

Feature should be independent of each other in order to avoid multicollinearity issues.
"""

# Finding tvalues and pvalues
c= display(model.tvalues,model.pvalues)

print("Admin:",float(6.017551e-01))
print("Marketing:",float(1.047168e-01))

"""Here, Beta0(Intercept) p_value ~ 1

Hypothesis testing of X variable by finding test_statistics and P_values for Beta1 i.e ifP_value < α=0.05, H0 is rejected

**Null Hypothesis as Beta1=0 (No Slope)**

**Alternate Hypthesis as Beta1≠0 (Some or significant Slope)** 

Note: Since the p-value is not less than 0.05 for Administration and Marketing features, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the sample data providing those features have any dependency towards the dependent variable.

**We are now going to perform Simple Linear Regression to separately check the dependency between those feature with Output variable**.

Let's first compare models by their R-Squared and p_values.
"""

admin_slr_model = sm.ols("profit~admin", data = df1).fit()
# Finding tvalues and pvalues
display(admin_slr_model.tvalues, admin_slr_model.pvalues)

# Finding rsquared values
admin_slr_model.rsquared , admin_slr_model.rsquared_adj

marketing_slr_model = sm.ols("profit~marketing", data = df1).fit()
# Finding tvalues and pvalues
display(marketing_slr_model.tvalues, marketing_slr_model.pvalues)

# Finding rsquared values
marketing_slr_model.rsquared , marketing_slr_model.rsquared_adj

"""NOTES:

1.The p_value of Administration is greater than 0.5 hence by the test of independence, we fail to reject Null Hypothesis.

2.We have found lack of evidence to state that administration has any dependency with respect to Profit atleast in this testing

**Calculating VIF ( Variance Inflation Factor ) to check the dependency between two input variables**
"""

# Method to calculate VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
x = df1[['marketing','admin','RnD']]
vif_data = pd.DataFrame()
vif_data['Features'] = x.columns
vif_data['VIF'] = [vif(x.values, i) for i in range(len(x.columns))]
vif_data.sort_values(by = ['VIF'])

"""**None of the variables has VIF>20, No Collinearity, but we cannot consider all varaibles in Regression equation**

As we can observe this test is not giving us much of an information to come up to an conclusion.

Let's try another approach that will justify our first test we conducted.

**Let's build two model in each model one of the feature will not be present and then we will come to an conclusion to remove which of the feature by comparing the model parameters**
"""

model_with_adm= sm.ols('profit~RnD+admin', data= df1).fit()     #RnD + Adminstaration
model_with_adm.summary()

model_with_marketing = sm.ols('profit~RnD+marketing', data= df1).fit()                #RnD + Marketing
model_with_marketing.summary()

"""Note : While building a model using Marketing we got good R-squared, R-squared adjusted, p_value(Test of Independence) and F-statistics as compared to model with Adminstration feature.

### **Model Validation**
*Comparing different models with respect to their Root Mean Squared Errors*
"""

RMSE = print('Adminstration Model=', np.sqrt(model_with_adm.mse_resid),'\n''Marketing Model=', np.sqrt(model_with_marketing.mse_resid))

"""**Let's compare the Root Mean Squared Error and check for the minimum value**"""

rmse_compare = {'Adminstration Model': np.sqrt(model_with_adm.mse_resid),'Marketing Model': np.sqrt(model_with_marketing.mse_resid)}
min(rmse_compare, key=rmse_compare.get)

"""### **Note: The Model that was build without using Marketing feature performed very well.**

It scored minimumn Root mean squared error and maximum R-squared and adjusted R-squared.

Now,we are going to rebuild the model by dropping the Administration feature
"""

final_model = sm.ols('profit~RnD+marketing', data=df1).fit()
final_model.summary()

"""### **Predicting values**"""

predicted = pd.DataFrame()
predicted['RnD'] = df1.RnD
predicted['marketing'] = df1.marketing
predicted['Profit'] = df1.profit
predicted['Predicted_Profit'] = pd.DataFrame(final_model.predict(predicted[['RnD','marketing']]))
predicted

"""### **Table containing R^2 value for each prepared model**"""

models={'Different_Models':['model1','model','Final_Model'],
        'R_squared':[model1.rsquared,model.rsquared,final_model.rsquared],
        'R_squared_adjusted':[model1.rsquared_adj,model.rsquared_adj,final_model.rsquared_adj],
       'RMSE':[np.sqrt(model1.mse_resid),np.sqrt(model.mse_resid),np.sqrt(final_model.mse_resid)]}

model_table=pd.DataFrame(models)
model_table